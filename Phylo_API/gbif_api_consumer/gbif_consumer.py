""" Module to query and retrieve occurrences of specific species of Gbif API """
import sys
import os
import threading
import requests
import re
import pandas as pd
import zipfile as zipf
import json
import inspect
import time
import datetime

# Serializers and models
from species_app.models import download_occurrences_date, markers
from species_app.serializers import download_ocurrences_date_serializer, markers_serializer

# Documentation imports
from typing import Union

# Module
from .exceptions import *

def gbif_consumer_master(scientific_name: str, api_specie_id: int = 2, api_user_id: Union[int, None]= None):
    """ 
    Holds all the flow of the module and returns the final result
        
    Attributes:
        scientific_name (str): scientific name of the target spcecie
        api_specie_id (int): identification code of a specie in our DB
        api_user_id (int|None): identification code the user that makes the request in our DB or None if is made by the server

    On complete: Call finish function to set the status
    """
    curr_date = datetime.date.today()

    try:
        # returns --> query_set => list -> tuple -> date
        last_download = list(download_occurrences_date.objects
        .filter(specie = api_specie_id).order_by("-download_date")
        .values_list('download_date', 'specie_id')[:1])[0][0]

        # If a specie is queried to update multiple times in one day this avoids to enter as a new specie
        if isinstance(last_download, datetime.timedelta) and last_download.days == 0:
            last_download = datetime.timedelta(days=1)

        days_from_download = curr_date - last_download
    except IndexError:
       days_from_download = curr_date - curr_date

    # Enters if the specie has not been updated in 60 days or does not has records
    if(days_from_download.days > 60 or days_from_download.days == 0):

        curr_occurrences = list(markers.objects
        .filter(specie_id = api_specie_id)
        .values_list('identification_id'))

        ocurrences_tidy = []
        for ocurrence in curr_occurrences:
            ocurrences_tidy.append(ocurrence[0])

        taxon_key = get_gbif_taxon_id(scientific_name)

        if taxon_key:
            resultSet = get_specie_occurrences(taxon_key, api_specie_id, api_user_id, ocurrences_tidy)
            download_id = resultSet[0]
            taxon_occurrences = resultSet[1]

            if not isinstance(resultSet, list):
                stand_by_thread("failed")
        else:
            stand_by_thread("not_found")

        return save_data(taxon_occurrences[500:], download_id, api_specie_id)

    else:
        stand_by_thread("recently_downloaded")

def stand_by_thread(status: str):
    """ 
    Sets the name of the thread and sleeps it for 10 minutes
        
    Attributes:
        status (str): Status name 

    On complete: Thread dies
    """
    curr_name = threading.current_thread().name
    threading.current_thread().name = f"{curr_name}:{status}"
    time.sleep(600)

def save_data(taxon_occurrences: pd.DataFrame, download_id: str, specie_id: int):
    """ 
    Persist the markers in the DB
        
    Attributes:
        taxon_occurrences (pd.DataFrame): Dataframe with the data
        download_id (str): Download id generated by Gbif API
        specie_id (int): Target specie of the markers

    On complete: Call finish function to set the status
    """
    ocurrences_parsed = markers_serializer(data = taxon_occurrences, many = True)

    register_save = download_ocurrences_date_serializer(data = {"download_id": download_id, "specie": specie_id})
    if register_save.is_valid():
        register_save.save()

        if ocurrences_parsed.is_valid():
            ocurrences_parsed.save()

    for error in register_save.errors:
        if register_save.errors[error][0].code == "unique":
            stand_by_thread("recently_downloaded")
    stand_by_thread("success")

def get_gbif_taxon_id(scientific_name: str) -> int:
    """ 
    Queries the Gbif api by a scientific name and return his taxon id
        
    Attributes:
        scientific_name (str): scientific name of the target spcecie

    Return:
        taxon_key (int|str): taxon id or empty string if is not found
    """
    taxon_request = requests.get(f"https://api.gbif.org/v1/species/match/?name={scientific_name}&strict=false")
    
    try:
        taxon_key = json.loads(taxon_request.text)['usageKey']
    except KeyError:
        taxon_key = ""

    return taxon_key

def get_specie_occurrences(taxon_key: int, specie_id: int, user_id: Union[int, None], curr_occurrences: list) -> list:
    """ 
    Holds the flow of the Gbif occurrence query
        
    Attributes:
        download_id (str): identification code of the download of the request
        taxon_occurrences_df (pd.Dataframe): dataframe with the occurrences

    Return:
        taxon_key (int|str): taxon id or empty string if is not found
        specie_id (int): identification code of the target specie in our DB
        user_id (int|None): identification code of the target user in our DB or None if is made by the server
        curr_occurrences (list): list of identification codes of the current stored occurrences of a specie in our DB
    """
    try:
        taxon_key = int(taxon_key)
        specie_id = int(specie_id)
        
        headers = {'Content-Type': 'application/json'}
        
        with open(os.path.join(sys.path[0], "gbif_api_consumer/query.json"), "r") as file:
            base_query = json.loads(file.read())
            
        base_query['predicate']['value'] = str(taxon_key)
        base_query = json.dumps(base_query)
        
        response = requests.post('https://api.gbif.org/v1/occurrence/download/request', headers=headers, data=base_query, auth=('robertomg', 'Phylotree_gbif'))
        download_id = response.text
        
        reg = r"https://api\.gbif\.org/v1/occurrence/download/\d{7}-\d{15}"
        pat = re.compile(reg)
        url_status = f'https://api.gbif.org/v1/occurrence/download/{download_id}'
        result = pat.fullmatch(url_status)
        if result is None:
            raise GbifIdToLarge

        api_success_response = check_download_status(url_status)
        
        download_occurrence_data(api_success_response, taxon_key)
        
        data = parse_occurrence_data(f"gbif_api_consumer/{taxon_key}.csv")
        
        taxon_occurrences_df = tidy_and_dict(data, specie_id, user_id, curr_occurrences)
        
        # Removes the download files
        os.remove(os.path.join(sys.path[0], f"gbif_api_consumer/{taxon_key}.csv"))
        os.remove(os.path.join(sys.path[0], f"gbif_api_consumer/{taxon_key}.zip"))

        return [download_id, taxon_occurrences_df]
        
    except ValueError:
        if "get_specie_occurrences" == inspect.trace()[-1][3]:
            return {"status": "taxon_invalid"}
        elif "set_axis" == inspect.trace()[-1][3]:
            return {"status": "taxon_not_found"}
        else:
            return {"status": "unknown"}
            
    except GbifIdToLarge:
        return {"status": "gbif_url_to_large"}
    
def check_download_status(url_status: str) -> requests.request:
    """ 
    Check when the download finishes
        
    Attributes:
        url_status (str): url of the download request

    Return:
        request (request): last request to check the status
    """
    flag = True
    while flag:
        time.sleep(10)
        status_req = requests.get(url_status)
        status = json.loads(status_req.text)['status']
        if status == "SUCCEEDED":
            flag = False
    return status_req
    
def download_occurrence_data(status_req: requests.request, taxon_key: int):
    """ 
    Unzips the downloaded file and sets his name
        
    Attributes:
        status_req (request): status download request that contains the download link
        taxon_key (int): taxon key used to set the CSV filename
    """
    filename = f"gbif_api_consumer/{taxon_key}.zip"
    download_link = json.loads(status_req.text)['downloadLink']
    
    with requests.get(download_link) as rq:
        with open(os.path.join(sys.path[0], filename), 'wb') as file:
            file.write(rq.content)
            
    with zipf.ZipFile(os.path.join(sys.path[0], filename), 'r') as target_file:
        target_file.filelist[0].filename = f"gbif_api_consumer/{taxon_key}.csv"
        target_file.extract(target_file.filelist[0])
        
def parse_occurrence_data(filename: str) -> pd.DataFrame:
    """ 
    Reads the CSV data file and get the required columns, drops incomplete registers, creates date and time fields throught splitting eventDate field
        
    Attributes:
        filename (str): local filename

    Return:
        final_df (pd.DataFrame): dataframe with the required data
    """
    with open(os.path.join(sys.path[0], filename), "r") as file:
            raw_specie_ocurrences = pd.read_csv(file, sep = "\t", usecols = [0, 1, 15, 17, 21, 22, 29,], na_values=" NaN") 

    tidy_specie_ocurrences = raw_specie_ocurrences.dropna()
    date_and_time = tidy_specie_ocurrences.eventDate.str.split("T", expand= True)
    date_and_time.columns = ['date', 'time']
    tidy_specie_ocurrences = tidy_specie_ocurrences.drop(columns = ['eventDate'])
    final_df = pd.concat([tidy_specie_ocurrences, date_and_time], axis = 1)
    return final_df
    
def tidy_and_dict(data: pd.DataFrame, speice_id: int, user_id: int, curr_records_ids: list) -> pd.DataFrame:
    """ 
    Sets the correct column names, drops the occurrences that already exists
        
    Attributes:
        data (pd.DataFrame): dataframe to tidy
        specie_id (int): identification code of the target specie in our DB
        user_id (int): identification code of the target user in our DB
        curr_records_ids (int): identifications of the occurrences of the specie already stored in the DB

    Return:
        data (dict): nested dictionaries with each row being one dictionary
    """
    data['marker_id'] = 0
    data['specie_id'] = speice_id
    data['user'] = user_id
    data = data.rename(columns={'decimalLongitude':'longitude',
                                'decimalLatitude':'latitude',
                                'stateProvince':'state',  
                                'countryCode':'country',
                                'time':'hour',
                                'datasetKey':'dataset_key',
                                'gbifID':'identification_id'}) 

    for index, row in data.iterrows():
        if row['identification_id'] in curr_records_ids:
            # by default inplace is false, that means that return a df without the specific row, if is true it deletes from current df directly
            data.drop(index, inplace = True)

    return data.to_dict(orient='records')